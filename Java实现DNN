DNN学习：转载自CSDN
程序员如何学习机器学习
对程序员来说，机器学习是有一定门槛的（这个门槛也是其核心竞争力），相信很多人在学习机器学习时都会为满是数学公式的英文论文而头疼，甚至可能知难而退。但实际上机器学习算法落地程序并不难写，下面是70行代码实现的反向多层（BP）神经网络算法，也就是深度学习。其实不光是神经网络，逻辑回归、决策树C45/ID3、随机森林、贝叶斯、协同过滤、图计算、Kmeans、PageRank等大部分机器学习算法都能在100行单机程序内实现（以后考虑分享出来）。

机器学习的真正难度在于它为什么要这么计算，它背后的数学原理是什么，怎么推导得来的公式，网上大部分的资料都在介绍这部分理论知识，却很少告诉你该算法的计算过程和程序落地是怎么样的，对于程序员来说，你需要做的仅是工程化应用，而不需要证明出一项新的数学计算方法。实际大部分机器学习工程师都是利用别人写好的开源包或者工具软件，输入数据和调整计算系数来训练结果，甚至很少自己实现算法过程。但是掌握每个算法的计算过程仍然非常重要，这样你才能理解该算法让数据产生了什么样的变化，理解算法的目的是为了达到什么样的效果。

本文重点探讨反向神经网络的单机实现，关于神经网络的多机并行化， Fourinone 提供非常灵活完善的并行计算框架，我们只需要理解透单机程序实现，就能构思和设计出分布式并行化方案，如果不理解算法计算过程，一切思路将无法展开。另外，还有卷积神经网络，主要是一种降维思想，用于图像处理，不在本文讨论范围。

延伸阅读：

机器学习开发者的现代化路径：不需要从统计学微积分开始
开发者成功使用机器学习的十大诀窍
机器学习温和指南
神经网络的计算过程
神经网络结构如下图所示，最左边的是输入层，最右边的是输出层，中间是多个隐含层，隐含层和输出层的每个神经节点，都是由上一层节点乘以其权重累加得到，标上“+1”的圆圈为截距项b，对输入层外每个节点：Y=w0*x0+w1*x1+…+wn*xn+b，由此我们可以知道神经网络相当于一个多层逻辑回归的结构。



（图片来自 UFLDL Tutorial ）

算法计算过程：输入层开始，从左往右计算，逐层往前直到输出层产生结果。如果结果值和目标值有差距，再从右往左算，逐层向后计算每个节点的误差，并且调整每个节点的所有权重，反向到达输入层后，又重新向前计算，重复迭代以上步骤，直到所有权重参数收敛到一个合理值。由于计算机程序求解方程参数和数学求法不一样，一般是先随机选取参数，然后不断调整参数减少误差直到逼近正确值，所以大部分的机器学习都是在不断迭代训练，下面我们从程序上详细看看该过程实现就清楚了。

神经网络的算法程序实现
神经网络的算法程序实现分为初始化、向前计算结果，反向修改权重三个过程。

1. 初始化过程

由于是n层神经网络，我们用二维数组layer记录节点值，第一维为层数，第二维为该层节点位置，数组的值为节点值；同样，节点误差值layerErr也是相似方式记录。用三维数组layer_weight记录各节点权重，第一维为层数，第二维为该层节点位置，第三维为下层节点位置，数组的值为某节点到达下层某节点的权重值，初始值为0-1之间的随机数。为了优化收敛速度，这里采用动量法权值调整，需要记录上一次权值调整量，用三维数组layer_weight_delta来记录，截距项处理：程序里将截距的值设置为1，这样只需要计算它的权重就可以了，

2. 向前计算结果

采用S函数1/(1+Math.exp(-z))将每个节点的值统一到0-1之间，再逐层向前计算直到输出层，对于输出层，实际上是不需要再用S函数的，我们这里将输出结果视为0到1之间的概率值，所以也采用了S函数，这样也有利于程序实现的统一性。

3. 反向修改权重

神经网络如何计算误差，一般采用平方型误差函数E，如下：



也就是将多个输出项和对应目标值的误差的平方累加起来，再除以2。实际上逻辑回归的误差函数也是这个，至于为什么要用这个函数来计算误差，它从数学上的合理性是什么，怎么得来的，这个我建议程序员们不想当数学家的话，先不去深究了，现在我们要做的是如何把这个函数E误差取它的最小值，需要对其进行求导，如果有些求导数学基础的话，倒可以尝试去推导下如何从函数E对权重求导得到下面这个公式的：



不会推导也没有关系，我们只需要运用结果公式就可以了，在我们的程序里用layerErr记录了E对权重求导后的最小化误差，再根据最小化误差去调整权重。

注意这里采用动量法调整，将上一次调整的经验考虑进来，避免陷入局部最小值，下面的k代表迭代次数，mobp为动量项，rate为学习步长：

Δw(k+1) = mobp*Δw(k)+rate*Err*Layer

也有很多使用下面的公式，效果上的差别不是太大：

Δw(k+1) = mobp*Δw(k)+(1-mobp)rate*Err*Layer

为了提升性能，注意程序实现是在一个while里面同时计算误差和调整权重，先将位置定位到倒数第二层（也就是最后一层隐含层）上，然后逐层反向调整，根据L+1层算好的误差来调整L层的权重，同时计算好L层的误差，用于下一次循环到L-1层时计算权重，以此循环下去直到倒数第一层（输入层）结束。

小结
在整个计算过程中，节点的值是每次计算都在变化的，不需要保存，而权重参数和误差参数是需要保存的，需要为下一次迭代提供支持，因此，如果我们构思一个分布式的多机并行计算方案，就能理解其他框架中为什么会有一个Parameter Server的概念。
